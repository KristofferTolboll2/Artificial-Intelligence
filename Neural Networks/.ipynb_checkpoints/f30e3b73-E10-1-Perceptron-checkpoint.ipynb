{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E10-1 Artificial Neuron Network:  Perceptron\n",
    "\n",
    "Perceptron is the simplest element of a neural network: one neuron connected with several dendroids (input) and one or more axons (output).\n",
    "Here we will create a Python model of it.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kristoffer/Desktop/schoolwork/Neural Networks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "[[0.38 0.19 0.  ]\n",
      " [0.17 0.31 0.  ]\n",
      " [0.29 0.54 0.  ]\n",
      " [0.89 0.55 1.  ]\n",
      " [0.78 0.36 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# load input data from diagnostic tests, first two numbers are features, the last is a label\n",
    "data = np.loadtxt('./perceptron.txt')\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features from the labels\n",
    "X = data[:, 0:2]\n",
    "print(X)\n",
    "\n",
    "y = data[:, -1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the labels back to a column\n",
    "n = len(data)\n",
    "y = y.reshape(n,1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the input data\n",
    "plt.figure()\n",
    "plt.title('Input data')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Perceptron\n",
    "Perceptron is the simplest type of artificial neural network.\n",
    "It simulates a biological neuron, which accepts input signals via its dendrites and passes the electrical signal to the cell body.<br>\n",
    "The artificial perceptron receives 'input signals' from the input training data set, which is then weighted and combined in a linear equation, used as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X values are the input, y values are the output\n",
    "# We try to find such weights of X, needed for precise calculation of y\n",
    "\n",
    "# Activation function calculates a value of firing the neuron\n",
    "# activation = sum(weight_i * x_i) + bias\n",
    "\n",
    "# Transfer function predicts the label\n",
    "# if activation >= 0.0 then prediction = 1.0 else prediction = 0.0\n",
    "\n",
    "# We reach the solution by iterations - epochs, trying to reduce the inaccuracy of the predicted output\n",
    "# After the first training iteration (epoch), an error is estimated, weights adjusted, and training repeted\n",
    "\n",
    "# Learning_rate determines how fast we advance to the solution, adjusting the weights, it is configurable\n",
    "# w = w + learning_rate * (expected_output - predicted_output) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function\n",
    "# FX = FX + X[0]*W[0] + X[1]*W[1]\n",
    "def predict(X, W, b):\n",
    "    FX = b   \n",
    "    for i in range(len(X)):\n",
    "        FX += X[i]*W[i] \n",
    "        \n",
    "        if FX >= 0.0: \n",
    "            active = 1.0 \n",
    "        else: \n",
    "            active = 0.0\n",
    "    return active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume random initial weights and a bias\n",
    "import random\n",
    "w1 = random.random()\n",
    "w2 = random.random()\n",
    "W = [w1, w2]\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = random.random()\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for correcting weights using stochastic gradient descent\n",
    "def train(X, y, l_rate, n_epoch):\n",
    "    W = [0.0 for i in range(len(X[0]))]\n",
    "    bias = -0.0 \n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch=%d' %(epoch))\n",
    "        sum_err = 0.0\n",
    "        i=0\n",
    "        # Loop over each row of the training data\n",
    "        for row in X:\n",
    "            print('\\tRow=', row, 'W =', W, 'bias=', bias)\n",
    "            \n",
    "            y_predicted = predict(row, W, bias)\n",
    "            # calculate the error as expected - predicted\n",
    "            err = y[i] - y_predicted\n",
    "            sum_err += err**2\n",
    "            print(\"\\t\\tExpected=%2d, Predicted=%2d, Error=%.2f, Cumulative error=%.2f\" %(y[i], y_predicted, err, sum_err))\n",
    "            \n",
    "            # Make corrections\n",
    "            # new bias\n",
    "            bias += l_rate * err        \n",
    "            # Loop over each weight in a row for updating it\n",
    "            n = len(row)\n",
    "            for j in range(n):\n",
    "                W[j] += l_rate * err * row[j]\n",
    "            # End of row        \n",
    "            i+=1\n",
    "        # End of epoch\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Stochastic Gradient Descent, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method requires two parameters\n",
    "# learning rate - limits the weight correction\n",
    "l_rate = 0.02\n",
    "# number of iterations through the data\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the activation function for each data row separately\n",
    "# Iterate through epochs with updated weights and bias\n",
    "weights = train(X, y, l_rate, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "k = 0.78*0.023 + 0.36*0.0178 - 0.02\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.38*0.023 + 0.19*0.017 -0.02\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Library Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neurolab\n",
    "import neurolab as nl\n",
    "# Dimensions of the inputs\n",
    "d1min, d1max, d2min, d2max = 0, 1, 0, 1\n",
    "inp1 = [d1min, d1max]\n",
    "inp2 = [d2min, d2max]\n",
    "inp = [inp1, inp2]\n",
    "\n",
    "# One output neuron, producing binary result\n",
    "outp = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = nl.net.newp(inp, outp)\n",
    "print(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the error of classification at each itteration\n",
    "err = perceptron.train(X, y, epochs=100, show=20, lr=0.03)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error\n",
    "plt.figure()\n",
    "plt.title('Training Error Progress')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(err)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the classifier on test datapoints\n",
    "print('\\nTest results:')\n",
    "data_test = [[0.38, 0.19], [0.4, 0.6], [0.7, 0.1]]\n",
    "for item in data_test:\n",
    "    print(item, '-->', perceptron.sim([item])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test With Other Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
