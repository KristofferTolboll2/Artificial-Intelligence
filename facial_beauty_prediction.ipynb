{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial_beauty_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KristofferTolboll2/Artificial-Intelligence/blob/master/facial_beauty_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbPuhV_oBeJy",
        "colab_type": "text"
      },
      "source": [
        "#Neural networks\n",
        "\n",
        "#Vanishing gradient problem\n",
        "A general problem, that occurs when training deep Neural Networks, is the vanishing gradient problem. The problem arrizes, when we ask ourself how to initialize weights of a neural network. This it also the intuition to use <b>ReLU</b> as our activation function <i>(for the hidden layer)</i> as opose to the <b>sigmoid function</b> (which has also been applied to this project). \n",
        "\n",
        "We know, that when we are using an optimizer like <b>Gradient Descent </b> we are trying to find the <b>Global minima</b> [<i>refrence</i>](https://docs.google.com/presentation/d/1Ili8Z04nu4uaanzM8mc4YpHQSz8NwhFvIB7-bxC8t1s/edit#slide=id.g871247f80c_0_9)\n",
        "\n",
        "The reson why we prefer to use <b>ReLU </b> in our hidden layers, is clear when we look at the derivative of the sigmoid function. \n",
        "\n",
        "![sigmoid derivative](https://i.stack.imgur.com/inMoa.png)\n",
        " \n",
        " If we examine the graph above, we can see, that the derivative of the <b>Sigmoid function </b> is always between <b>0 - 0.25</b>\n",
        "\n",
        " If we remeber the <b>Gradient Descent </b> Formula from before, we know, that we have to plugin in the old weight minus the learning right times the derivative of loss divided by the derivative of the weight. \n",
        "\n",
        "But what happens, when we have many layers in our Neural Network?\n",
        "Our inituition is, that each layer effects the next layer, so how does Gradient Descent come into this?\n",
        "\n",
        "We have a concept called <b>The Chain Rule </b>, which is one of the most fundamental concepts in multi-layered neural networks. This mathematical concept.\n",
        "Since our network will have a lot of derivatives to calculate, <b>The Chain Rule </b> allows us to take the derivative of the seperate functions and <b>\"chain\"</b> them together. Below i have provided a very simple example of this.\n",
        "\n",
        " ![Chain Rule](https://miro.medium.com/max/1092/1*e6Epzbmngh2a50WUrKleUA.png)\n",
        "\n",
        "\n",
        " The problem occurs, when we use the <b>Sigmoid Function </b> in this context, since we know, that the derivative of a sigmoid function is always ranging between <b>0-0.25</b> the end result, can end up being a very small number, which means, that the activations and the weights of the initial layers , will only be updated very minimally in larger networks. \n",
        "Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fYYkk7VBjGY",
        "colab_type": "text"
      },
      "source": [
        "#Exploding gradient problem\n",
        "Another problem, that can cause our neural netowrk to decrease in performance is, what is known as the <b>Exploding Gradient Problem </b> this also occurs during the <b>backpropagation phase </b>. Unlike the <b>Vanishing Gradient Problem </b> where the gradient <i>\"vanishes\"</i> and the acivation and weights are interchangeable for each <b>Epoch </b>, the Exploding Gradient problem is, when it is not possible to reach the global minima point during the gradient descent process. This can result in a very unstable network where the gradient will in term <b>\"bounce\" or explode </b> \n",
        "Signs that an exploding gradient problem could be occuring could be:\n",
        "<li>The model is unable to get traction on your training data (e.g. poor loss).\n",
        "<li>The model is unstable, resulting in large changes in loss from update to \n",
        "update.\n",
        "<li>The model weights quickly become very large during training.\n",
        "\n",
        "To prevent the Exploding Gradient problem from occuring we can use several different techniques. However during the development process of this neural network, we did not experience the <b>Exploding Gradient Problem </b>, on the main reasons, why this did not occur, might be due to the fact, that a faily small batch size was used for this model. This was both done by specifying the batch size, when defyning our constants, and using the <b>Adam optimizer </b> which derives from the stochastic gradient descent algorithm. \n",
        "<br/>\n",
        "<br />\n",
        "This ensures, that we only have random samples in our batch-size, which will result, in the weights not being adjusted to quickly, and leaving the model unstable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-QAHUd6FyYI",
        "colab_type": "text"
      },
      "source": [
        "#Convelutional neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBRpHdAdK-Ie",
        "colab_type": "text"
      },
      "source": [
        "Convelutional neural networks are a type of Neural Networks, that is typically used for image and video analysis. The reson, why this type of network is used, is because of it's ability to detect patterns. The major difference between a regular multilayered neural network, is it's use of <b>Convelutional layers </b>. <br />\n",
        "<br />\n",
        "Where as a regular ANN would typically have some specefic input features for each neuron in the input layer. When using a CNN we interept the pixels of an image as inputs. To each of these inputs we apply a filter, that can detect patterns in the image. The filter is mapped over each pixel. This process is called the <b>Convelution</b>. The specific values, that are mapped on to the image pixels, are the <b>weights </b>, where typically we will add a bias value aswell (<b>not displayed on image</b>), which will hope us scale our values, to suit our dataset. <br />\n",
        "![conv](https://static.packt-cdn.com/products/9781789138139/graphics/57b07b69-9550-4cda-a798-0578c8e30c74.png)\n",
        "\n",
        "Well maintaining the same terms, as a regular neural network, it is simple, the way of the operations, and the input data, that is different. <br />\n",
        "similarly, we will also apply an activation function, to the sum of weights, acivation values and bias. \n",
        "\n",
        "# Max pooling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBDShsBFzpN",
        "colab_type": "text"
      },
      "source": [
        "# ResNet50 and Residual netural networks<\n",
        "ResNet50 is a popular neural network, that is typically used for <b>Image classification</b> inside neural networks.\n",
        "<br />\n",
        "The ResNet50 is made to solve a recurring problem, when creating <b>CNN's</b>, the fact, that deeper neural networks accuracy easily gets saturated, meaning they can easily get overfitted. \n",
        "<b><i>Residual neural networks </i></b>work by skipping some of the layers during training and testing. This helps prevent the [<i>vanishing gradient problem </i> ](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
        ", by reusing the activation values until the adjacent layer learns its correct weights. \n",
        "\n",
        "Another cause might be the <b>degradation problem </b>\n",
        "\n",
        "<br />\n",
        "The practical reason, that <b> Residual Networks </b>are supirior in some cases are due to the facts, that they implement a tactic called a <b>Residual Block </b>, what this does in term, is to skip some of the Fully Connected Layers, and just put the output on to the next layer <i>(demontrated below)</i>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQxhazXLhsfG",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/868/0*sGlmENAXIZhSqyFZ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QYvq0cWhvmL",
        "colab_type": "text"
      },
      "source": [
        "<i>As the figure above describe we have two connected weighted layers in our neural network, they each have the <b>ReLU </b> activation function, which is applied as F(X).</i>\n",
        "\n",
        "The intuition behind this technique comes from the fact, that it gets increasingly hard training neural networks with more layers <i>(deeper NN)</i> The more layers, the more advanced and premices we can get, but only to a certain degree. As mentioned above, very deep neural networks have two high coherence to have the performance improved. \n",
        "\n",
        "When we skip over layers. We look at the activation from the previous layer, and let that derterminte the activation of the current layer. <b>This ensures, that we can add more layers, without decreasing performance because we use a regularisation technique (Dropout)</b>. This ensures, that layers, that are not improving our performance are skipped, and will not have a negative impact on our Neural Networks performance. \n",
        "\n",
        "In term <b>Regularization</b> is something, that we add to our <b>loss function </b> to controll for large weights <i>(see more on the use of dropout layers below)</i> <br />\n",
        "In this context we can regulate for the large weights, by skipping over the neurons. \n",
        "\n",
        "Imagine you play a game with multi players in the same team, the game is about transfer a message from first player though other players, for example player A talks to player B, layer B talks to player C and so all, the last player can be repeat the same message as original message from A, that is good. But information can be lost between players because noise, quality of player sound, quality of hearing of receiver,...So if some how player A talk directily to layer C or D also, and it cause less information lost. That is the reason Resedual are used in RestNet architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dJLdDAcIggl",
        "colab_type": "code",
        "outputId": "0391fe3e-2a4d-4392-cf20-88b41f9c569c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6wnh42yN8W",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShsxvYQ28PSb",
        "colab_type": "code",
        "outputId": "be91743a-55d3-4eef-960b-40db06d44225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  SCUT-FBP5500-Database-Release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_65f20bI8dLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir('machine_learning')\n",
        "#os.chdir('/gdrive/My Drive/ML/AWS')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjjBBdiI358",
        "colab_type": "code",
        "outputId": "2611475d-d1b1-472d-deee-fdd25cd88ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoWI0GLs8mE9",
        "colab_type": "code",
        "outputId": "4d5ca885-b17e-467f-d2aa-13b3e12f0c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dudE-dAYHR5",
        "colab_type": "text"
      },
      "source": [
        "Clone the git repo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C8VnRSRJOvu",
        "colab_type": "code",
        "outputId": "c4690021-18fe-4247-906f-56902ad3f299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!git clone https://github.com/HCIILAB/SCUT-FBP5500-Database-Release"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SCUT-FBP5500-Database-Release'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 226 (delta 8), reused 0 (delta 0), pack-reused 210\u001b[K\n",
            "Receiving objects: 100% (226/226), 1.57 MiB | 11.71 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVg0UQO7YNk8",
        "colab_type": "text"
      },
      "source": [
        "Download the zip file, with google colabs <b>gdown </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86R6ZlRvrJB1",
        "colab_type": "code",
        "outputId": "b6f70ad5-67e1-47d3-af10-b74de8b140b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!gdown --id 1w0TorBfTIqbquQVd6k3h_77ypnrvfGwf4/0AFDM2lMe2hCyuyknEWnABAXNw4etrahfdphEZz8FnS3mEcJgfLfyNk4/0AFDM2lMe2hCyuyknEWnABAXNw4etrahfdphEZz8FnS3mEcJgfLfyNk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1w0TorBfTIqbquQVd6k3h_77ypnrvfGwf4/0AFDM2lMe2hCyuyknEWnABAXNw4etrahfdphEZz8FnS3mEcJgfLfyNk4/0AFDM2lMe2hCyuyknEWnABAXNw4etrahfdphEZz8FnS3mEcJgfLfyNk\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hk3eOyiYcaq",
        "colab_type": "text"
      },
      "source": [
        "Unzip the zip folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdoNA1FUtV-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip SCUT-FBP5500_v2.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfaBTpzRYf_u",
        "colab_type": "text"
      },
      "source": [
        "Check out the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdNbJzbuQTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls SCUT-FBP5500_v2/Images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-MPz-B4zAdx",
        "colab_type": "text"
      },
      "source": [
        "# Facial beauty prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbP3PL-AYisH",
        "colab_type": "text"
      },
      "source": [
        "Making our imports... We will address each of the imports in their respective cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1HALWw-zGK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.optimizers import SGD \n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "from keras.layers import Dropout\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import Conv2D, Input, MaxPool2D,Flatten, Dense, Permute, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import adam\n",
        "import keras\n",
        "import cv2\n",
        "import sys\n",
        "import dlib\n",
        "import os.path\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from skimage.io import imread\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4HM3rErz1-m",
        "colab_type": "text"
      },
      "source": [
        "Define CONSTS <br />\n",
        "https://docs.google.com/presentation/d/18BcievZBg7wX7nUplPX4-D5Y43wEMxLujoLfR6G2-gE/edit#slide=id.g86ff0b06fe_0_48\n",
        "<br />\n",
        "<br />\n",
        "<b>NUM_CLASSES</b> This defines the amount of output neurons we have in our neural network. This was initially set to 1, but was later changed to 5 due to better acurracy score. \n",
        "<br />\n",
        "<br />\n",
        "<b>DROPOUT </b> A dropout layer is, is a technique used in deep learning, which \"ignores\" certain neurons, which are randomly selected. Ignored meaning, that they are not included in the <b>forward -or backward propagation </b>\n",
        "The reason we use dropout rates is to prevent <b>overfitting </b> as demonstrated in the [<i>google slide </i> ](https://docs.google.com/presentation/d/18BcievZBg7wX7nUplPX4-D5Y43wEMxLujoLfR6G2-gE/edit#slide=id.g86ff0b06fe_0_48)during the initial training phase, the model was very overfit.   \n",
        "This is due to the fact, that in a fully connected layer, all the neurons are very co-dependent of each other <i>(especially in a deep NN with many neurons like Resnet-50)</i>\n",
        "<br />\n",
        "[<i>Visualization</i>](https://docs.google.com/presentation/d/18BcievZBg7wX7nUplPX4-D5Y43wEMxLujoLfR6G2-gE/edit#slide=id.g86ff0b06fe_0_53)\n",
        "The value og <b>0.5 </b> means, that there is a \n",
        "\n",
        "<b> BATCH_SIZE </b> <br/>\n",
        "The batch size dertermintes how many training samples are used in each iteration. As i explained above we are using a relatively small batch_size for this project. I have previously tried with a larger batch size, but i often let to the network being <b>overfit</b> the appropriate batch size seems to be about 16. This process is also often called <b>mini-batch mode </b> where your batch size is greater than one, but stiller a relatively small number, to prevent overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX0A9L6rz6ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 5\n",
        "DROPOUT = 0.5\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4REbXVbnyj1c",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jaTQlnStX-y",
        "colab_type": "code",
        "outputId": "bb95cce6-dae5-4d18-d745-fe62fd2679f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "ratings = pd.read_excel('SCUT-FBP5500_v2/All_Ratings.xlsx')\n",
        "ratings.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rater</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Rating</th>\n",
              "      <th>original Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>CF1.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>CF10.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>CF100.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>CF101.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>CF102.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rater   Filename  Rating  original Rating\n",
              "0      1    CF1.jpg       3              NaN\n",
              "1      1   CF10.jpg       3              NaN\n",
              "2      1  CF100.jpg       1              NaN\n",
              "3      1  CF101.jpg       2              NaN\n",
              "4      1  CF102.jpg       3              NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_8-Y_W7quqD",
        "colab_type": "text"
      },
      "source": [
        "5500 images rated by 60 different raters = 330.000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs0KikfvZsul",
        "colab_type": "code",
        "outputId": "ca96d8ac-dc73-4e6a-db2a-f487af4e5e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ratings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(330000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPk743U7LC96",
        "colab_type": "text"
      },
      "source": [
        "# Observing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9WPNFVhq_51",
        "colab_type": "text"
      },
      "source": [
        "Here we are training to make a data analysis to see, if there is some bias in our dataset. We can confirm, that we have 60 different <i>Raters</i> each have a number/id assigned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUfInlybLRoc",
        "colab_type": "code",
        "outputId": "b518d57b-d23b-4158-8434-28ee5bbe37f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "ratings.Rater.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGGeejH4sSUM",
        "colab_type": "text"
      },
      "source": [
        "This could appears to be [<b>Gausian distributed </b>](https://wiki.analytica.com/index.php?title=Normal_distribution)\n",
        ". This is a good sign. Our data should be a generel representation of the population, where the far most are place in the middle tier (3) fewer in the higher and lower tier (2 and 4), and fewest at the very top and bottom (1 and 5). Below is a graph that visualize this.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IAVQVitNlpl",
        "colab_type": "code",
        "outputId": "fbde378f-1791-4aae-97bc-2f5f75152aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "ratings.Rating.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    159328\n",
              "2     67634\n",
              "4     62272\n",
              "5     20972\n",
              "1     19794\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f43FWM6utINN",
        "colab_type": "text"
      },
      "source": [
        "We want to find the mean for each 'Rater' to see if there could by any bias, for people rating higher or lower. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIsrGfg7R0Vq",
        "colab_type": "code",
        "outputId": "18135963-0e6b-4212-c3df-9230bf205036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "ratings.loc[ratings.Rater == 1].Rating.mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5176363636363637"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEefGJ1xSlqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_ratings = []\n",
        "raters = len(ratings.Rater.unique())\n",
        "\n",
        "for rater in range(1, (raters +1)):\n",
        "  mean_ratings.append(ratings.loc[ratings.Rater == rater].Rating.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-NePLqQUuyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_ratings.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EYr-qwCtNoF",
        "colab_type": "text"
      },
      "source": [
        "Here we sort the average ratings per Rater, and see if there are any major differences in between, what was rated. Overall there is some differences, but the ratings in general are pretty similar. This is good, and could be an indicator, that our raters, have had the same intuition, of what the <b>\"scale\"</b> should be used. The major issue, with this project, is that beauty will also have a sense of subjectivity, which makes this a dificult classification problem.<br/>\n",
        "The fact that, the mean of the ratings are similar, is an indicator, that our dataset is useful. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q55qt8S7TNqa",
        "colab_type": "code",
        "outputId": "0e853582-0fcb-452a-fb63-097c4a8c83f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.bar(range(len(mean_ratings)), mean_ratings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 60 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R55GeLJotzkP",
        "colab_type": "text"
      },
      "source": [
        "Visualization of our <b>Gausian distributed</b> data. <br /> <br />\n",
        "We can see, that our dataset is almost completly gausian distributed. Not only does this mean, we have a good dataset, it also makes our machine learning task easier. This is because gausian data is simpler to work with. \n",
        "<li> It's mean, median and mode are all same\n",
        "<li>The entire distribution can be specified using just two parameters- mean and variance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0s8NBaCLlDt",
        "colab_type": "code",
        "outputId": "8da2a8a2-3229-43e1-d50f-8b22b067a34e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(ratings.Rating)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc81b308080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uStd_cEfX6mO",
        "colab_type": "text"
      },
      "source": [
        "Hey\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4qZ_y-uMvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_width = 350\n",
        "img_height = 350\n",
        "channels = 3 #RGB\n",
        "sample_dir = 'SCUT-FBP5500_v2/Images/'\n",
        "nb_samples = len(os.listdir(sample_dir))\n",
        "input_shape = (img_width, img_height, channels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-8OSg_LuVq2",
        "colab_type": "code",
        "outputId": "e12c6dc2-5c42-4545-a017-2726dd696f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file_names = ratings.groupby('Filename').size().index.tolist()\n",
        "\n",
        "labels = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    df = ratings[ratings['Filename'] == file_name]\n",
        "    count = Counter(df['Rating']).most_common(1)[0][0]\n",
        "    #returns a list with a tuple, the displays the most common ratings for each file \n",
        "    #We use the Counter object from the collection module to achieve this task\n",
        "    score = round(df['Rating'].mean(), 2)\n",
        "    labels.append({'Filename': sample_dir+file_name, 'most_common': count, 'score': score})\n",
        "\n",
        "labels_df = pd.DataFrame(labels)\n",
        "labels_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>most_common</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SCUT-FBP5500_v2/Images/AF1.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>2.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SCUT-FBP5500_v2/Images/AF10.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>3.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SCUT-FBP5500_v2/Images/AF100.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>2.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SCUT-FBP5500_v2/Images/AF1000.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>3.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SCUT-FBP5500_v2/Images/AF1001.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>3.73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Filename  most_common  score\n",
              "0     SCUT-FBP5500_v2/Images/AF1.jpg            3   2.33\n",
              "1    SCUT-FBP5500_v2/Images/AF10.jpg            4   3.43\n",
              "2   SCUT-FBP5500_v2/Images/AF100.jpg            3   2.90\n",
              "3  SCUT-FBP5500_v2/Images/AF1000.jpg            4   3.97\n",
              "4  SCUT-FBP5500_v2/Images/AF1001.jpg            4   3.73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9OiMCUzZrk",
        "colab_type": "text"
      },
      "source": [
        "Seed is used for getting reproduced the same data each time. When we incoorporate the random state from the <b> train_test_split </b> function in SkLearn, this will ensure, that we get the same split of traning and test data, each time our code runs. You could say that it ensures, that the data is <b>derterministic</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq-tNCIRvd55",
        "colab_type": "code",
        "outputId": "7d469722-49e7-4915-c637-a8ee476a442f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "seed = 29 \n",
        "files = [sample_dir+i for i in os.listdir(sample_dir)]\n",
        "#ten fold cross validation.\n",
        "#We divide our data in 10 \"blocks\", where one block\n",
        "#will make up the testing data\n",
        "train_files, test_files = train_test_split(files, test_size=0.1, random_state=seed)\n",
        "print(len(train_files))\n",
        "print(len(test_files))\n",
        "print(train_files[:5])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4950\n",
            "550\n",
            "['SCUT-FBP5500_v2/Images/AF118.jpg', 'SCUT-FBP5500_v2/Images/AM46.jpg', 'SCUT-FBP5500_v2/Images/AF1407.jpg', 'SCUT-FBP5500_v2/Images/AF1755.jpg', 'SCUT-FBP5500_v2/Images/AF1741.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejlmBicHv4eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_image_generator(files, batch_size):\n",
        "    NB_BATCH = len(files)//batch_size\n",
        "    while True:\n",
        "        for i in range(0, NB_BATCH):\n",
        "            X_feature = np.empty(shape=(batch_size, img_width, img_height, channels), dtype=np.float)\n",
        "            y_label = np.empty(shape=(batch_size, 1), dtype=np.float)\n",
        "            #loop all image current patch\n",
        "            for k, j in enumerate(range(i*batch_size, (i+1)*batch_size)):\n",
        "                X_feature[k] = imread(files[j]) / 255.\n",
        "                y_label[k] = labels_df[labels_df.Filename==files[j]].score.values.astype('float')\n",
        "            yield X_feature, y_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP41Wumdyo18",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veJMKLCvzSKX",
        "colab_type": "text"
      },
      "source": [
        "Define custom load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_KWJXzy1I6",
        "colab_type": "text"
      },
      "source": [
        "Load pretrained restnest50\n",
        "<br />\n",
        "<br />\n",
        "To elaborate the <b>Dropout</b> class from keras is a way to drop neurons during the training phase to put emphasize on the neurons, that have more impact on the output.  \n",
        "<br >\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiNW1xHtzYWR",
        "colab_type": "code",
        "outputId": "e50d8934-e974-4912-a31c-91c3cb42bda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "resnet = ResNet50(include_top=False, pooling='avg')\n",
        "model = Sequential()\n",
        "model.add(resnet)\n",
        "model.add(Dropout(DROPOUT))\n",
        "#model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "model.add(Dense(1))\n",
        "#free the weight of restnest except the FC layer\n",
        "#model.layers[0].trainable = False\n",
        "print (model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 23,589,761\n",
            "Trainable params: 23,536,641\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fogtduk_EDDK",
        "colab_type": "text"
      },
      "source": [
        "Above the summary of the model is specified. We have the first layer, which is our pretrained <i>resnet50 model</i>, and then our dropout layer, where we in the end have our single output neuron. <br/>\n",
        "\n",
        "As you can see, the <i>resnet50</i> model has 23587712 parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZvPaq6r0U9v",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf5w8bUTrUA5",
        "colab_type": "text"
      },
      "source": [
        "Here we are setting our options for the optimzer, which is applied, when the model is training. We have previously worked with Gradient Descent, but for this project i decided to use the <i><b>Adam</b></i> optimzer, which is a version of the <i><b>Stochsatic gradient Descent</b></i> algorithm.  <br/>\n",
        "\n",
        "Since we are using a deep <b>CNN</b> we will have a lot of input features, and this means a lot of computation during our backpropagation phase. Using a regular Gradient Descent the computation would be almost impossible.  \n",
        "\n",
        "<b>Stochastic Gradient Descent</b> will sample random inputs and apply the gradient descent algorithm on those. This is especially useful, when we have <b>Redundant data</b> because the seperate data points will have less influence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLCJ1s3y0lzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"mse-{epoch:02d}-{val_loss:.4f}.h5\"\n",
        "#verbose will display the progress bar, when the model is training\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "#this is hyper parameter turnig that reduce the learning rate until reach min_lr\n",
        "\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n",
        "                                         factor=0.1,\n",
        "                                         patience=1,\n",
        "                                         cooldown=2,\n",
        "                                         min_lr=0.00001,\n",
        "                                         verbose=1)\n",
        "#define EarlyStopping that monitor the validation loss after 3 epochs, \n",
        "#the validation loss is not improved, the training will be stopped\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
        "#during training we can use callback to handle \n",
        "#during training ve can use checkpointm reduce learning rate and apply early stoping\n",
        "callback_list = [checkpoint, reduce_learning_rate, early_stop]\n",
        "\n",
        "#compile the model chose which optomizer used and loss function\n",
        "model.compile(loss='mse', optimizer=Adam(lr=0.0001))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klmzvHizyhEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB_xwBhAyryq",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1rNPEAm9Uxz",
        "colab_type": "text"
      },
      "source": [
        "<b>val_loss</b> is the value of cost function for your cross-validation /testing data and <b>loss</b> is the value of cost function for your training data. <br /> <br />\n",
        "On validation data, neurons using drop out do not drop random neurons. The reason is that during training we use drop out in order to add some noise for avoiding over-fitting. During calculating cross-validation, we are in the recall phase and not in the training phase. We use all the capabilities of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS4S6pa509hr",
        "colab_type": "code",
        "outputId": "b34b6221-2256-4ac1-e051-e8c7e605bce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\n",
        "history = model.fit_generator(generator=custom_image_generator(train_files, BATCH_SIZE),\n",
        "                              steps_per_epoch=len(train_files)//BATCH_SIZE,\n",
        "                              validation_data=custom_image_generator(test_files, BATCH_SIZE),\n",
        "                              validation_steps=len(test_files)//BATCH_SIZE,\n",
        "                              callbacks=callback_list,\n",
        "                              epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 60/309 [====>.........................] - ETA: 1:44:36 - loss: 1.0833"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hw75ED31Ksy",
        "colab_type": "text"
      },
      "source": [
        "# Plot training result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3_znbrwPeZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training loss')\n",
        "plt.plot(epochs, loss, 'red', label='Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training validation loss')\n",
        "plt.plot(epochs, val_loss, 'blue', label='Training validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bsNtn_4yvmj",
        "colab_type": "text"
      },
      "source": [
        "# Make Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_imOunX2gTS",
        "colab_type": "text"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2oel1F92EQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "best_model = load_model('mse-10-0.1101.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb62kUpOg_YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.models import load_model\n",
        "\n",
        "#best_model = load_model('mse-13-0.0591.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI9H-DzEJ85f",
        "colab_type": "text"
      },
      "source": [
        "make prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7LnLeUFEFs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = best_model.predict_generator(generator=custom_image_generator(test_files, BATCH_SIZE),\n",
        "                                         steps=len(test_files)//BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT0a6_rqEKuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.empty(shape=(len(test_files), 1), dtype=np.float)\n",
        "for i, k in enumerate(test_files):\n",
        "    y_test[i] = labels_df[labels_df.Filename==k].score.values.astype('float')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM-urDg1fib2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(y_test[:test_pred.shape[0]], test_pred)\n",
        "plt.plot(y_test, y_test, 'ro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usBXKjAWEOpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(y_test[:test_pred.shape[0]], test_pred)\n",
        "plt.plot(y_test, y_test, 'ro')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLxJRAYhEVcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['font.size'] = 9\n",
        "plt.rcParams['figure.figsize'] = (9,9)\n",
        "\n",
        "from random import randint\n",
        "nb_test_samples = len(test_files)\n",
        "nb_rows, nb_cols = 5, 5\n",
        "\n",
        "def check_preidction():\n",
        "    for k in range(nb_rows * nb_cols):\n",
        "        i = randint(0, nb_test_samples - 1)\n",
        "        x = imread(test_files[i]) / 255\n",
        "        y = labels_df[labels_df.Filename==test_files[i]].score.values\n",
        "        predicted = best_model.predict(x.reshape((1,) + x.shape))\n",
        "        plt.subplot(nb_rows, nb_cols, k+1)\n",
        "        plt.imshow(x)\n",
        "        plt.title(\"p:%.2f a:%.2f\" % (predicted[0][0]*2, y*2))\n",
        "        plt.axis('off')\n",
        "\n",
        "check_preidction()\n",
        "#model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TLLqe5Nvegu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYh86qUKjDVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = randint(0, 550)\n",
        "x = imread(test_files[i])\n",
        "y = labels_df[labels_df.Filename==test_files[i]].score.values\n",
        "plt.imshow(x)\n",
        "plt.title(f'Score is {y*2}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S140ERWDYXRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install urllib2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_X-JjlplUfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "def predict_picture(img_url):\n",
        "  img = urlopen(img_url)\n",
        "  a = plt.imread(img, 0) / 255\n",
        "  predicted = best_model.predict(a.reshape((1,) + a.shape))\n",
        "  print(predicted *2)\n",
        "  plt.imshow(a)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6KSxKkvlafv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_picture(\"https://img.freepik.com/free-photo/portrait-white-man-isolated_53876-40306.jpg?size=350&ext=jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilqZ-IITldsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILRCvjY9bDeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_picture(\"https://pbs.twimg.com/media/DYAfnOoWAAA3T7D.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x16JrsYZbZKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://arunponnusamy.com/files/mmod_human_face_detector.dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2tw8WdKTN-W",
        "colab_type": "text"
      },
      "source": [
        "It is better if we crop the image to get the head (or face) because the the image for traning is just the head with full face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFSPz4W6WPEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "model_path = 'mmod_human_face_detector.dat'\n",
        "cnn_face_detector = dlib.cnn_face_detection_model_v1(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7wvmT61T-Vd",
        "colab_type": "text"
      },
      "source": [
        "We should change shape of the image for using dlib. it is a powerful libary for predic face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZCxhYDuTpxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_good_shape(image_name):\n",
        "    orgiginal_image = cv2.imread(image_name)\n",
        "    if orgiginal_image.shape[0] > 1280:\n",
        "        good_shape = (1280, orgiginal_image.shape[1] * 1280 / orgiginal_image.shape[0])\n",
        "    elif orgiginal_image.shape[1] > 1280:\n",
        "        good_shape = (im0.shape[0] * 1280 / im0.shape[1], 1280)\n",
        "    elif orgiginal_image.shape[0] < 640 or orgiginal_image.shape[1] < 640:\n",
        "        good_shape = (orgiginal_image.shape[0] * 2, orgiginal_image.shape[1] * 2)\n",
        "    else:\n",
        "        good_shape = orgiginal_image.shape[0:2]\n",
        "    #New we have  new image with better shape\n",
        "    new_image = cv2.resize(orgiginal_image, (int(good_shape[1]), int(good_shape[0])))\n",
        "    return new_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpcyzzPQ0SBQ",
        "colab_type": "text"
      },
      "source": [
        "In this cell, we try to scale an arbitrary image, into a format, that is better suited for our model. The problem is often, that a lot of portraits have a background, which can give a bad prediction, because it will add some "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGWtU_STloTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_face(imgage_name):\n",
        "    new_image = make_good_shape(imgage_name)\n",
        "    detections = cnn_face_detector(new_image, 0)\n",
        "    for i, d in enumerate(detections):\n",
        "        face_location = [d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()]\n",
        "        croped_image = new_image[face_location[1]:face_location[3], face_location[0]:face_location[2], :]\n",
        "        resized_image = cv2.resize(croped_image, (350, 350))\n",
        "        normed_image = np.array([(resized_image - 127.5) / 127.5])\n",
        "        cv2.rectangle(new_image, (face_location[0], face_location[1]), (face_location[2], face_location[3]), (0, 255, 0), 3)\n",
        "        #cv2.putText(new_image, str('%.2f' % (out)), (face_location[0], face_location[3]), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        #                1, (0, 0, 255), 2)\n",
        "    new_face_image = new_image[face_location[1]: face_location[3],face_location[0]: face_location[2]]\n",
        "    final_croped_image = cv2.imwrite('out-'+image_name, new_face_image)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIbaDSwZVc8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_face('https://pbs.twimg.com/media/DYAfnOoWAAA3T7D.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4OFKcB1lS0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}